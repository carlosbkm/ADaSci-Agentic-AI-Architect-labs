{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f00416",
   "metadata": {},
   "source": [
    "# Integrating Multiple Agentic Frameworks\n",
    "In this tutorial, we explore how to combine LangGraph, a flexible framework for building agentic workflows, with AutoGen, a powerful agent system from Microsoft, to build an intelligent chatbot that:\n",
    "\n",
    "* Supports persistent memory between interactions\n",
    "* Uses AutoGen's reasoning abilities to handle complex tasks\n",
    "* Leverages LangGraph's features like task routing, entrypoints, and streaming output\n",
    "\n",
    "---\n",
    "Why integrate LangGraph with other frameworks?\n",
    "LangGraph provides strong abstractions for stateful, multi-step applications. Integrating external agent frameworks like AutoGen enables:\n",
    "\n",
    "\n",
    "*   Modular agent design across ecosystems\n",
    "*   Seamless use of LangGraph features (memory, streaming, persistence)\n",
    "*   Support for hybrid multi-agent workflows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d623879",
   "metadata": {},
   "source": [
    "### Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d0c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install autogen-agentchat[gemini]~=0.2 langgraph litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638dc4a",
   "metadata": {},
   "source": [
    "### Step 2: Configure OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0143d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from google.colab import userdata\n",
    "import os\n",
    "import litellm # Imported as an architecture hint although not used directly. Autogen uses it under the hood.\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "OAI_CONFIG_LIST = [\n",
    "    {\n",
    "        \"model\": \"gpt-35-turbo\",\n",
    "        \"api_key\": \"your OpenAI Key goes here\",\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gemini-2.0-flash\",\n",
    "        \"api_key\": os.environ.get(\"GEMINI_API_KEY\"),\n",
    "        \"api_type\": \"google\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713dc247",
   "metadata": {},
   "source": [
    "### Step 3: Define AutoGen Agents\n",
    "\n",
    "We’ll use AutoGen's AssistantAgent and UserProxyAgent. These agents can auto-respond and run code when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061efae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "import os\n",
    "\n",
    "\n",
    "config_list_gemini  = [\n",
    "    {\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "        \"api_key\": os.environ.get(\"GEMINI_API_KEY\"),\n",
    "        \"api_type\": \"google\"\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"cache_seed\": 42,\n",
    "    \"config_list\": config_list_gemini,\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "autogen_agent = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"web\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8181bd",
   "metadata": {},
   "source": [
    "### Step 4: Create the LangGraph Workflow\n",
    "\n",
    "Let’s build a LangGraph graph that delegates reasoning to AutoGen, with support for short-term memory and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32478a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_openai_messages, BaseMessage\n",
    "from langgraph.func import entrypoint, task\n",
    "from langgraph.graph import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "@task\n",
    "def call_autogen_agent(messages: list[BaseMessage]):\n",
    "    # convert to openai-style messages\n",
    "    messages = convert_to_openai_messages(messages)\n",
    "    response = user_proxy.initiate_chat(\n",
    "        autogen_agent,\n",
    "        message=messages[-1],\n",
    "        # pass previous message history as context\n",
    "        carryover=messages[:-1],\n",
    "    )\n",
    "    # get the final response from the agent\n",
    "    content = response.chat_history[-1][\"content\"]\n",
    "    return {\"role\": \"assistant\", \"content\": content}\n",
    "\n",
    "\n",
    "# add short-term memory for storing conversation history\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "def workflow(messages: list[BaseMessage], previous: list[BaseMessage]):\n",
    "    messages = add_messages(previous or [], messages)\n",
    "    response = call_autogen_agent(messages).result()\n",
    "    return entrypoint.final(value=response, save=add_messages(messages, response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68041855",
   "metadata": {},
   "source": [
    "###  Step 5: Run the Graph\n",
    "\n",
    "Initialize a Config with Thread ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the thread ID to persist agent outputs for future interactions\n",
    "# highlight-next-line\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "for chunk in workflow.stream(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Which numbers between 1 and 50 are divisible by 7?\",\n",
    "        }\n",
    "    ],\n",
    "    # highlight-next-line\n",
    "    config,\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c506efa2",
   "metadata": {},
   "source": [
    "### **Step 6: Continue the Conversation**\n",
    "Because we're using LangGraph’s persistence layer, we can pick up the thread in future runs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in workflow.stream(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Multiply the last number by 3\",\n",
    "        }\n",
    "    ],\n",
    "    # highlight-next-line\n",
    "    config,\n",
    "):\n",
    "    print(chunk)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
